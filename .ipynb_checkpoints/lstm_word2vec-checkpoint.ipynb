{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "13153\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "vocab=set(model.wv.index2word)\n",
    "print(model.wv.index2word.index(\"with\"))\n",
    "print(len(vocab))\n",
    "print(type(model.wv.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first we are going to read data \n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "labeled_train = pd.read_csv('./data/labeledTrainData.tsv',delimiter='\\t',quoting=3)\n",
    "train,test=tts(labeled_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining a function to convert reviews to list of words\n",
    "#stop_words=stopwords.words(\"english\")\n",
    "def review_to_listofwords(raw_review):\n",
    "    #stripping html out of review\n",
    "    review_text=BeautifulSoup(raw_review,\"html.parser\").getText()\n",
    "    #getting rid of all the punctuation\n",
    "    review_text=re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #converting in to lower case\n",
    "    review_text=review_text.lower()\n",
    "    #tokenizing to words\n",
    "    words=review_text.split()\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function converts a review into list of sentences where each sentence is composed of a list of words\n",
    "def review_to_sentences(review,tokenizer):\n",
    "    #raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    if len(review) > 0:\n",
    "        rv_words = review_to_listofwords(review)\n",
    "        for word in rv_words:\n",
    "            if word in vocab:\n",
    "                sentences.append(model.wv.index2word.index(word))\n",
    "            else:\n",
    "                sentences.append(0)\n",
    "        #sentences.append(sent_index)        \n",
    "    return sentences        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(dataframe,tokenizer):\n",
    "    train_reviews = dataframe['review']\n",
    "    sentences=[]\n",
    "    for review in train_reviews:\n",
    "        sentences.append(review_to_sentences(review,tokenizer))\n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 24.9 ms, total: 1min 43s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "%time train_sentences=build_dataset(train,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4618, 174, 8, 237, 0, 0, 10209, 10209]\n",
      "22500 22500 22500\n"
     ]
    }
   ],
   "source": [
    "print(review_to_sentences('hello world i am hardik khandelwal. yo yo!!',tokenizer))\n",
    "print(len(train_sentences),len(train),len(train['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "# into our TensorFlow and Keras models\n",
    "vector_dim=300\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13153, 300)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix])\n",
    "model1.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Conv1D(filters=300, kernel_size=3, padding='same', activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "model1.add(LSTM(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         3945900   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 300)         270300    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 4,937,701\n",
      "Trainable params: 4,937,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500 125\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences),len(train_sentences[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 237, 0, 0, 82, 3, 25, 391]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_sentences(\"I am hardik khandelwal. First of his name.\",tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length=200\n",
    "X_train = sequence.pad_sequences(train_sentences, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "22500/22500 [==============================] - 611s 27ms/step - loss: 0.3730 - acc: 0.8329\n",
      "Epoch 2/3\n",
      "22500/22500 [==============================] - 609s 27ms/step - loss: 0.2202 - acc: 0.9153\n",
      "Epoch 3/3\n",
      "22500/22500 [==============================] - 610s 27ms/step - loss: 0.1339 - acc: 0.9541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3dc6f725f8>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, train['sentiment'], epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 0 ns, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%time test_sentences=build_dataset(test,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.16%\n"
     ]
    }
   ],
   "source": [
    "X_test = sequence.pad_sequences(test_sentences, maxlen=max_review_length)\n",
    "scores = model1.evaluate(X_test, test['sentiment'], verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "test_data= pd.read_csv(\"./data/testData.tsv\",delimiter=\"\\t\", quoting=3)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 37.4 ms, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%time test_data_sentences=build_dataset(test_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_real = sequence.pad_sequences(test_data_sentences, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model1.predict(X_test_real,batch_size=64,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(predictions.shape)\n",
    "p=predictions.reshape(25000)\n",
    "\n",
    "output = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":p} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"lstm_rnn.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#epochs=3  accuracy= 93.334%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
