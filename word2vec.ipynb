{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create word2vec fron our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeledTrainData.tsv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'sentiment' 'review']\n",
      "['id' 'review']\n"
     ]
    }
   ],
   "source": [
    "#first we are going to read data \n",
    "labeled_train = pd.read_csv('./data/labeledTrainData.tsv',delimiter='\\t',quoting=3)\n",
    "unlabeled_train = pd.read_csv('./data/unlabeledTrainData.tsv',delimiter='\\t',quoting=3)\n",
    "print(labeled_train.columns.values)\n",
    "print(unlabeled_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining a function to convert reviews to list of words\n",
    "#stop_words=stopwords.words(\"english\")\n",
    "def review_to_listofwords(raw_review):\n",
    "    #stripping html out of review\n",
    "    review_text=BeautifulSoup(raw_review,\"html.parser\").getText()\n",
    "    #getting rid of all the punctuation\n",
    "    review_text=re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #converting in to lower case\n",
    "    review_text=review_text.lower()\n",
    "    #tokenizing to words\n",
    "    words=review_text.split()\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function converts a review into list of sentences where each sentence is composed of a list of words\n",
    "def review_to_sentences(review,tokenizer):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for s in raw_sentences:\n",
    "        if len(s) > 0:\n",
    "            sentences.append(review_to_listofwords(s))\n",
    "    return sentences        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(dataframe,tokenizer):\n",
    "    train_reviews = dataframe['review']\n",
    "    sentences=[]\n",
    "    for review in train_reviews:\n",
    "        sentences=sentences+review_to_sentences(review,tokenizer)\n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardik/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/hardik/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "#we will tokenize our reviews into sentences using punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "labeled_sentences=build_dataset(labeled_train,tokenizer)\n",
    "unlabeled_sentences=build_dataset(labeled_train,tokenizer)\n",
    "sentences=labeled_sentences+unlabeled_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Hyperparameters for word2vec model</B>:<br> <br>\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results. <br> <br>\n",
    "Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model. <br> <br>\n",
    "Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.<br> <br>\n",
    "Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).<br> <br>\n",
    "Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.<br> <br>\n",
    "Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 22:46:09,213 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-01-09 22:46:09,219 : INFO : collecting all words and their counts\n",
      "2018-01-09 22:46:09,220 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-09 22:46:09,287 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2018-01-09 22:46:09,342 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2018-01-09 22:46:09,399 : INFO : PROGRESS: at sentence #30000, processed 671314 words, keeping 30034 word types\n",
      "2018-01-09 22:46:09,454 : INFO : PROGRESS: at sentence #40000, processed 897814 words, keeping 34348 word types\n",
      "2018-01-09 22:46:09,511 : INFO : PROGRESS: at sentence #50000, processed 1116962 words, keeping 37761 word types\n",
      "2018-01-09 22:46:09,567 : INFO : PROGRESS: at sentence #60000, processed 1338403 words, keeping 40723 word types\n",
      "2018-01-09 22:46:09,624 : INFO : PROGRESS: at sentence #70000, processed 1561579 words, keeping 43333 word types\n",
      "2018-01-09 22:46:09,678 : INFO : PROGRESS: at sentence #80000, processed 1780886 words, keeping 45714 word types\n",
      "2018-01-09 22:46:09,739 : INFO : PROGRESS: at sentence #90000, processed 2004995 words, keeping 48135 word types\n",
      "2018-01-09 22:46:09,798 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2018-01-09 22:46:09,854 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2018-01-09 22:46:09,919 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2018-01-09 22:46:09,979 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2018-01-09 22:46:10,040 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2018-01-09 22:46:10,099 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2018-01-09 22:46:10,167 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2018-01-09 22:46:10,246 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2018-01-09 22:46:10,320 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2018-01-09 22:46:10,397 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2018-01-09 22:46:10,468 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2018-01-09 22:46:10,535 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2018-01-09 22:46:10,598 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2018-01-09 22:46:10,652 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2018-01-09 22:46:10,722 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2018-01-09 22:46:10,778 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2018-01-09 22:46:10,840 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2018-01-09 22:46:10,896 : INFO : PROGRESS: at sentence #270000, processed 5999168 words, keeping 74218 word types\n",
      "2018-01-09 22:46:10,955 : INFO : PROGRESS: at sentence #280000, processed 6223909 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,017 : INFO : PROGRESS: at sentence #290000, processed 6447077 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,072 : INFO : PROGRESS: at sentence #300000, processed 6668822 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,135 : INFO : PROGRESS: at sentence #310000, processed 6895779 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,190 : INFO : PROGRESS: at sentence #320000, processed 7114991 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,247 : INFO : PROGRESS: at sentence #330000, processed 7333809 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,309 : INFO : PROGRESS: at sentence #340000, processed 7557463 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,366 : INFO : PROGRESS: at sentence #350000, processed 7781085 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,427 : INFO : PROGRESS: at sentence #360000, processed 7999950 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,490 : INFO : PROGRESS: at sentence #370000, processed 8221068 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,567 : INFO : PROGRESS: at sentence #380000, processed 8442425 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,642 : INFO : PROGRESS: at sentence #390000, processed 8668136 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,710 : INFO : PROGRESS: at sentence #400000, processed 8884784 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,779 : INFO : PROGRESS: at sentence #410000, processed 9107348 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,838 : INFO : PROGRESS: at sentence #420000, processed 9328499 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,896 : INFO : PROGRESS: at sentence #430000, processed 9550879 words, keeping 74218 word types\n",
      "2018-01-09 22:46:11,952 : INFO : PROGRESS: at sentence #440000, processed 9773932 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,012 : INFO : PROGRESS: at sentence #450000, processed 9997826 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,065 : INFO : PROGRESS: at sentence #460000, processed 10221127 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,132 : INFO : PROGRESS: at sentence #470000, processed 10444168 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,187 : INFO : PROGRESS: at sentence #480000, processed 10667917 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,262 : INFO : PROGRESS: at sentence #490000, processed 10892775 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,323 : INFO : PROGRESS: at sentence #500000, processed 11118453 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,377 : INFO : PROGRESS: at sentence #510000, processed 11339041 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,443 : INFO : PROGRESS: at sentence #520000, processed 11553753 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,504 : INFO : PROGRESS: at sentence #530000, processed 11774180 words, keeping 74218 word types\n",
      "2018-01-09 22:46:12,521 : INFO : collected 74218 word types from a corpus of 11841448 raw words and 533102 sentences\n",
      "2018-01-09 22:46:12,522 : INFO : Loading a fresh vocabulary\n",
      "2018-01-09 22:46:12,593 : INFO : min_count=40 retains 13153 unique words (17% of original 74218, drops 61065)\n",
      "2018-01-09 22:46:12,595 : INFO : min_count=40 leaves 11389692 word corpus (96% of original 11841448, drops 451756)\n",
      "2018-01-09 22:46:12,641 : INFO : deleting the raw counts dictionary of 74218 items\n",
      "2018-01-09 22:46:12,644 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2018-01-09 22:46:12,645 : INFO : downsampling leaves estimated 8390634 word corpus (73.7% of prior 11389692)\n",
      "2018-01-09 22:46:12,645 : INFO : estimated required memory for 13153 words and 300 dimensions: 38143700 bytes\n",
      "2018-01-09 22:46:12,699 : INFO : resetting layer weights\n",
      "2018-01-09 22:46:12,909 : INFO : training model with 4 workers on 13153 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-01-09 22:46:13,930 : INFO : PROGRESS: at 1.31% examples, 547358 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:14,936 : INFO : PROGRESS: at 2.67% examples, 558001 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:15,940 : INFO : PROGRESS: at 4.04% examples, 562774 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:16,954 : INFO : PROGRESS: at 5.45% examples, 566685 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:17,959 : INFO : PROGRESS: at 6.76% examples, 563228 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:18,966 : INFO : PROGRESS: at 8.12% examples, 564171 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:19,991 : INFO : PROGRESS: at 9.50% examples, 564375 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-09 22:46:20,993 : INFO : PROGRESS: at 10.88% examples, 566063 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:22,000 : INFO : PROGRESS: at 12.26% examples, 567089 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 22:46:23,015 : INFO : PROGRESS: at 13.67% examples, 568287 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:24,018 : INFO : PROGRESS: at 15.04% examples, 568554 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:25,022 : INFO : PROGRESS: at 16.42% examples, 569292 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:26,023 : INFO : PROGRESS: at 17.81% examples, 570577 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:27,036 : INFO : PROGRESS: at 19.19% examples, 570704 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:28,045 : INFO : PROGRESS: at 20.59% examples, 571438 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:29,052 : INFO : PROGRESS: at 21.97% examples, 571747 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:30,062 : INFO : PROGRESS: at 23.35% examples, 571911 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:31,067 : INFO : PROGRESS: at 24.74% examples, 572232 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:32,079 : INFO : PROGRESS: at 26.15% examples, 572623 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:33,081 : INFO : PROGRESS: at 27.52% examples, 572957 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:34,088 : INFO : PROGRESS: at 28.91% examples, 573410 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:35,098 : INFO : PROGRESS: at 30.31% examples, 573391 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:36,102 : INFO : PROGRESS: at 31.66% examples, 573268 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:37,113 : INFO : PROGRESS: at 33.05% examples, 573316 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:38,118 : INFO : PROGRESS: at 34.44% examples, 573493 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:39,121 : INFO : PROGRESS: at 35.81% examples, 573403 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:40,122 : INFO : PROGRESS: at 37.18% examples, 573651 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:41,140 : INFO : PROGRESS: at 38.57% examples, 573739 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:42,149 : INFO : PROGRESS: at 39.99% examples, 573984 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:43,150 : INFO : PROGRESS: at 41.36% examples, 574177 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:44,159 : INFO : PROGRESS: at 42.74% examples, 574187 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:45,164 : INFO : PROGRESS: at 44.15% examples, 574517 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:46,165 : INFO : PROGRESS: at 45.52% examples, 574465 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:46:47,175 : INFO : PROGRESS: at 46.90% examples, 574428 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:48,177 : INFO : PROGRESS: at 48.23% examples, 574142 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:49,185 : INFO : PROGRESS: at 49.61% examples, 573985 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:50,185 : INFO : PROGRESS: at 50.95% examples, 573761 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 22:46:51,200 : INFO : PROGRESS: at 52.21% examples, 572390 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-09 22:46:52,221 : INFO : PROGRESS: at 53.53% examples, 571565 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:53,237 : INFO : PROGRESS: at 54.60% examples, 568228 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:54,244 : INFO : PROGRESS: at 55.79% examples, 566369 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:46:55,254 : INFO : PROGRESS: at 56.94% examples, 564374 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:46:56,270 : INFO : PROGRESS: at 58.20% examples, 563389 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:46:57,274 : INFO : PROGRESS: at 59.59% examples, 563699 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 22:46:58,276 : INFO : PROGRESS: at 60.83% examples, 562785 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:46:59,280 : INFO : PROGRESS: at 62.18% examples, 562799 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:00,284 : INFO : PROGRESS: at 63.41% examples, 561773 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:01,294 : INFO : PROGRESS: at 64.77% examples, 561901 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:47:02,303 : INFO : PROGRESS: at 66.18% examples, 562300 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:03,307 : INFO : PROGRESS: at 67.56% examples, 562614 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:04,309 : INFO : PROGRESS: at 68.73% examples, 561275 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:05,314 : INFO : PROGRESS: at 69.96% examples, 560213 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:06,328 : INFO : PROGRESS: at 71.35% examples, 560576 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-09 22:47:07,332 : INFO : PROGRESS: at 72.70% examples, 560627 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:08,333 : INFO : PROGRESS: at 74.07% examples, 560864 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 22:47:09,335 : INFO : PROGRESS: at 75.44% examples, 561061 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-09 22:47:10,341 : INFO : PROGRESS: at 76.79% examples, 561094 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:11,345 : INFO : PROGRESS: at 78.13% examples, 561132 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:12,353 : INFO : PROGRESS: at 79.51% examples, 561367 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:47:13,360 : INFO : PROGRESS: at 80.89% examples, 561596 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:14,376 : INFO : PROGRESS: at 82.27% examples, 561737 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:15,378 : INFO : PROGRESS: at 83.63% examples, 561794 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 22:47:16,382 : INFO : PROGRESS: at 85.00% examples, 561946 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:17,405 : INFO : PROGRESS: at 86.38% examples, 562025 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:18,406 : INFO : PROGRESS: at 87.76% examples, 562302 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:19,426 : INFO : PROGRESS: at 89.13% examples, 562388 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:20,436 : INFO : PROGRESS: at 90.47% examples, 562236 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:47:21,441 : INFO : PROGRESS: at 91.85% examples, 562455 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:22,476 : INFO : PROGRESS: at 93.25% examples, 562523 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:23,482 : INFO : PROGRESS: at 94.61% examples, 562630 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-09 22:47:24,483 : INFO : PROGRESS: at 96.00% examples, 562851 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:25,491 : INFO : PROGRESS: at 97.35% examples, 562832 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:26,492 : INFO : PROGRESS: at 98.69% examples, 562861 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 22:47:27,381 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-09 22:47:27,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-09 22:47:27,413 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-09 22:47:27,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-09 22:47:27,418 : INFO : training on 59207240 raw words (41954326 effective words) took 74.5s, 563171 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                          size=num_features, min_count = min_word_count,\n",
    "                          window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 22:54:46,286 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-01-09 22:54:46,437 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-01-09 22:54:46,438 : INFO : not storing attribute syn0norm\n",
      "2018-01-09 22:54:46,439 : INFO : not storing attribute cum_table\n",
      "2018-01-09 22:54:46,995 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardik/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('outstanding', 0.7961982488632202),\n",
       " ('exceptional', 0.7388363480567932),\n",
       " ('superb', 0.7252383828163147),\n",
       " ('terrific', 0.6861687898635864),\n",
       " ('fantastic', 0.6683037281036377),\n",
       " ('amazing', 0.6594833731651306),\n",
       " ('brilliant', 0.6451787948608398),\n",
       " ('awesome', 0.6322755813598633),\n",
       " ('incredible', 0.6180087327957153),\n",
       " ('marvelous', 0.617539644241333)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"excellent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardik/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('abysmal', 0.7746913433074951),\n",
       " ('horrendous', 0.7522616982460022),\n",
       " ('appalling', 0.7396042346954346),\n",
       " ('terrible', 0.7288037538528442),\n",
       " ('amateurish', 0.7272131443023682),\n",
       " ('horrid', 0.71351158618927),\n",
       " ('laughable', 0.7053291201591492),\n",
       " ('awful', 0.7022461891174316),\n",
       " ('uninspired', 0.6944233775138855),\n",
       " ('dreadful', 0.6936764121055603)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"atrocious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardik/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gorgeous', 0.6907073259353638),\n",
       " ('beautiful', 0.6399940252304077),\n",
       " ('elegant', 0.6288249492645264),\n",
       " ('delightful', 0.6285260915756226),\n",
       " ('radiant', 0.6140733957290649),\n",
       " ('barbara', 0.613771378993988),\n",
       " ('ellen', 0.6056407690048218),\n",
       " ('sweet', 0.6018452048301697),\n",
       " ('clara', 0.5908852219581604),\n",
       " ('charming', 0.58653724193573)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
